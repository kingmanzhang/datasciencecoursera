---
title: "DataScienceCapstone"
author: "Xingmin Aaron Zhang"
date: "1/23/2019"
output: html_document
---


# Section 1

## Goal:
We will explore the Swiftkey dataset to develop some intutitions about this project.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Download data

We will download the dataset directly from Coursera website. We can see that we have blogs, news and twitter texts for four languages. We will work on the English texts.
```{r}
setwd("~/git/datasciencecoursera")
fileurl = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
destfile = "Coursera-Swiftkey.zip"
if (!file.exists(destfile)) {
    download.file(fileurl, destfile = destfile)
}
# show files in the zip folder
unzip(destfile, list = TRUE)
```


```{r echo=FALSE}
## unzip file
if (!file.exists("final")){
  unzip(destfile)
}
```

# Data cleaning and exploratory analysis
We count the number of lines for three English corpus: twitter, news and blogs. From the graph we can see that blogs and news have roughly similar line counts while twitter corpus has twice line counts. 

```{r echo=FALSE, message=FALSE, fig.align='center', fig.width=4, fig.height=4}
corpusnames <- c("twitter", "news", "blogs")
EN.TWITTER.PATH = "final/en_US/en_US.twitter.txt"
EN.NEWS.PATH = "final/en_US/en_US.news.txt"
EN.BLOGS.PATH = "final/en_US/en_US.blogs.txt"

paths <- c(EN.TWITTER.PATH, EN.NEWS.PATH, EN.BLOGS.PATH)
names(paths) <- corpusnames

linecount <- function(paths) {
    counts <- vector(mode = "integer", length = length(paths))
    for (i in 1:length(paths)) {
        filecon <- file(paths[i], "r")
        counts[i] <- length(readLines(filecon, skipNul = TRUE))
        close(filecon)
    }
    counts
}
linecounts <- linecount(paths)
names(linecounts) <- corpusnames

df <- data.frame(type = corpusnames, linecount = linecounts)

library(tidyverse)
ggplot(df) + geom_bar(aes(x = type, y= linecount), stat = "identity") + ggtitle("total line counts for each type of corpuses") + theme(plot.title = element_text(hjust = 0.5)) + xlab("")
```

Before we do more sophisticated analysis, we select subsets of data from each file to reduce memory requirement. Since we have at least ~ 1 million lines for each file, we can propably get a good sense of the corpuses from 1% of the entire sample.
```{r echo=FALSE, cache=TRUE}
corpus_subset <- function(corpus_name = "twitter", percent = 0.001) {
    LINE_COUNT <- linecounts[corpus_name]
    readOrNot <- rbinom(LINE_COUNT, 1, percent)
    subset_lines <- vector(mode = "character", length = sum(readOrNot))
    i = 1
    j = 1
    filecon <- file(unname(paths[corpus_name]), "r")
    while (i <= LINE_COUNT) {
      newline = readLines(filecon, n = 1, skipNul = TRUE)
      if (readOrNot[i]){
        subset_lines[j] = newline
        j = j + 1
      }
      i = i + 1
    }
    close(filecon)
    subset_lines
}

subsetAndSave <- function(percent) {
    en.twitter.subset <- corpus_subset("twitter", percent)
    en.news.subset <- corpus_subset("news", percent)
    en.blogs.subset <- corpus_subset("blogs", percent)

    # We will save this for future use.
    if (!dir.exists("final/en_subsets")) {
         dir.create("final/en_subsets")
    }

    write(en.twitter.subset, file = "final/en_subsets/twitter_subset.txt")
    write(en.news.subset, file = "final/en_subsets/news_subset.txt")
    write(en.blogs.subset, file = "final/en_subsets/blogs_subset.txt")
}
```

```{r echo=FALSE}
# load subsets
en.twitter.subset <- readLines(con <- file("final/en_subsets/twitter_subset.txt"))
close(con)
en.news.subset <- readLines(con <- file("final/en_subsets/news_subset.txt"))
close(con)
en.blogs.subset <- readLines(con <- file("final/en_subsets/blogs_subset.txt"))
close(con)
```


Next, we are going to count the words for each corpus. We performed some cleanings: removed unnecessary white spaces, removed numbers, converted to lower case and removed punctuations.
```{r message=FALSE, echo=FALSE}
library(tm)
#We first import the subsets into separate corpuses.
twitter_corpus<- VCorpus(VectorSource(en.twitter.subset))
news_corpus <- VCorpus(VectorSource(en.news.subset))
blog_corpus <- VCorpus(VectorSource(en.blogs.subset))

corpuses <- list(twitter_corpus, news_corpus, blog_corpus)
preprocess <- function(corpus) {
  corpus <- tm_map(corpus, removePunctuation, preserve_intra_word_contractions = TRUE,
                  preserve_intra_word_dashes = TRUE)
  corpus <- tm_map(corpus, removeNumbers)
  #corpus <- tm_map(corpus, removeWords, stopwords(kind = "en"))
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus
}
processedCorpuses <- lapply(corpuses, preprocess)
dtms <- lapply(processedCorpuses, function(x){DocumentTermMatrix(x)})
df$docCounts <- sapply(dtms, function(x){dim(x)[1]})
df$uniqueWordCounts <- sapply(dtms, function(x){dim(x)[2]})
df$totalWordCount <- sapply(dtms, function(x){sum(x)})
```

First we looked at total words counts for each corpuses subset. From the subset of each corpuses, we can see that news have the most word count, twitter has the least even though it has most lines, and blogs are in the middle.
```{r echo=FALSE, message=FALSE, fig.align='center', fig.cap="word count for each corpus subset", fig.width=6, fig.height=3}
p1 <- ggplot(df) + geom_bar(aes(x = type, y = totalWordCount), stat = "identity") + xlab("") + ylab("count") + ggtitle("total word count") + theme(plot.title = element_text(hjust = 0.5))
p2 <- ggplot(df) + geom_bar(aes(x = type, y = uniqueWordCounts), stat = "identity") + xlab("") + ylab("count") + ggtitle("unique word count") + theme(plot.title = element_text(hjust = 0.5))

library(gridExtra)
grid.arrange(p1, p2, nrow = 1)
```


For each document/line, blogs or news have roughly similar number of words, while twitter has the least, which is totally reasonable.
```{r echo=FALSE, fig.align='center', fig.cap="word count per document for each corpus subset", fig.width=6, fig.height=3}
p3 <- ggplot(df) + geom_bar(aes(x = type, y = totalWordCount/docCounts), stat = "identity") + xlab("") + ggtitle("average word count") + theme(plot.title = element_text(hjust = 0.5))
p4 <- ggplot(df) + geom_bar(aes(x = type, y = uniqueWordCounts/docCounts), stat = "identity") + xlab("") + ggtitle("average unique word count") + theme(plot.title = element_text(hjust = 0.5))

grid.arrange(p3, p4, nrow = 1)
```

## N-gram analysis
In the next section, we are going to calculate the frequent N-gram from each corpus. 
```{r echo=FALSE}
nGramTokenizer <- function(n) { 
    function(x) {
        unlist(lapply(ngrams(words(x), n), paste, collapse = " "), use.names = FALSE)
    }   
}

oneGramTokenizer = nGramTokenizer(1)
twoGramTokenizer = nGramTokenizer(2)
threeGramTokenizer = nGramTokenizer(3)
    
```

We now start to explore n-gram distributions. We will combine the subsets of all twitters, blogs and news as one document because we only care about the combined result. 
```{r echo=FALSE}
combinedDocuments <- c(en.twitter.subset, en.news.subset, en.blogs.subset)
collapseToOne <- paste(combinedDocuments, collapse = " ")
combinedCorpus <- VCorpus(VectorSource(collapseToOne))
combinedCorpusProcessed <- preprocess(combinedCorpus)
dtm1 <- DocumentTermMatrix(combinedCorpusProcessed, list(tokenize = oneGramTokenizer, wordLengths=c(1, Inf)))
dtm2 <- DocumentTermMatrix(combinedCorpusProcessed, list(tokenize = twoGramTokenizer))
dtm3 <- DocumentTermMatrix(combinedCorpusProcessed, list(tokenize = threeGramTokenizer))

#dtm1 <- lapply(processedCorpuses, function(x){DocumentTermMatrix(x, list(tokenize = oneGramTokenizer, wordLengths=c(1, Inf)))})

#dtm2 <- lapply(processedCorpuses, function(x){DocumentTermMatrix(x, list(tokenize = twoGramTokenizer))})

#dtm3 <- lapply(processedCorpuses, function(x){DocumentTermMatrix(x, list(tokenize = threeGramTokenizer))})

```

Most frequent 15 1-grams:
```{r echo=FALSE}
mostFreqNGram <- function(x) {
    #dtmComb <- c(x[[1]], x[[2]], x[[3]])
    wordList <- colnames(x)
    freq <- colSums(as.matrix(x))

    wordFreqOneGram <- data.frame(wordList, freq)
    wordFreqOneGram %>% arrange(-freq)
}


onegram <- mostFreqNGram(dtm1)
onegram %>% head(n = 15)
```

Most frequent 2-grams:
```{r echo=FALSE}
bigram <- mostFreqNGram(dtm2) 
bigram %>% head(n = 15)
```

Most frequent 3-grams:
```{r echo=FALSE}
trigram <- mostFreqNGram(dtm3) 
trigram %>% head(n = 15)
```

## Conclusions

From our exploratory analysis, we can conclude that:

* There are more twitter lines than news or blogs
* News have more total words and total unique words, followed by blogs and twitter.
* Blogs have the most total words and unique words per document, closely followed by news. While twitters have the lease total words and unique words per document.
* We also showed the most frequent words, word pairs, and word triplets. 

Going forward, we can use the N-gram frequency to make predictions on the next input.


# Section 2

## Goals
Build a n-gram model to predict the next word

## Prediction with data frame implementation
The idea of the predictive model is using bigram to predict word based on previous 1-word, and using trigram to predict next word based on previous bi-gram.

One straightward way to implement the idea is using a data frame to save all 1,2 and 3-grams. In this dataframe, there will be three columns for individual words. The first column is just 1-grams words. The second column is the second word of bigrams, so that concatenating it with the word in the first column gives a bigram. This rule extends to the third column and trigrams. 

```{r}
onegram_dataframe <- onegram %>% rename(word1 = wordList) %>% mutate(word1 = as.character(word1))
bigram_dataframe <- bigram %>% separate(wordList, c("word1", "word2"), " ")
trigram_dataframe <- trigram %>% separate(wordList, c("word1", "word2", "word3"), " ")

resource <- onegram_dataframe %>% inner_join(bigram_dataframe, by = "word1") %>% inner_join(trigram_dataframe, by = c("word1", "word2"))
```

```{r}
require(tm)
require(slam)
require(hash)
require(triebeard)
require(tidyverse)
# preprocess corpus 
preprocess <- function(corpus) {
  corpus <- tm_map(corpus, removePunctuation, preserve_intra_word_contractions = TRUE,
                  preserve_intra_word_dashes = TRUE)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, stemDocument)
  #corpus <- tm_map(corpus, removeWords, stopwords(kind = "en"))
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus
}

nGramTokenizer <- function(n) { 
    function(x) {
        unlist(lapply(ngrams(words(x), n), paste, collapse = " "), use.names = FALSE)
    }   
}

# trim end word to convert n-grams to (n-1)-grams
trimEndWord <- function(x) {
  splitted <- strsplit(x, " ")
  sapply(splitted, function(x) {paste(x[1 : length(x) - 1], collapse = " ")})
}

# this extends the hash key lookup by allowing vector input
valuesVector <- function(x, h) {
  v <- vector(mode = "numeric", length = length(x))
  for (i in 1:length(x)) {
    y <- h[[x[i]]]
    if (is.null(y)) {
      v[i] = 0
    } else {
      v[i] = y
    }
    # if(has.key(x[i], h)) {
    #   v[i] = h[[x[i]]]
    # } else {
    #   v[i] = 0
    # }
  }
  v
}

# update hash: 
# put if key is not present
# update value by adding new value if key is present 
updateCounts <- function(h, keys, values) {
  if (length(keys) != length(values)) {
    stop("keys and values have different length")
  }
  currentValues <- valuesVector(keys, h)
  .set(h, keys, currentValues + values)
  h
}

# process specified file, update n-gram count
freqCount <- function(h, file, ngrams, batch_size) {
  LINE_PROCESSED = 0
  fcon <- file(file, "r")
  batch <- vector(mode = "character", length = batch_size)
  batch_i <- 1
  line <- readLines(fcon, n = 1, skipNul = TRUE)
  while(length(line) != 0) {
    LINE_PROCESSED = LINE_PROCESSED + 1
    batch[batch_i] <- line
    batch_i = batch_i + 1
    line = readLines(fcon, n = 1, skipNul = TRUE)
    if (batch_i > batch_size | length(line) == 0) {
      message(paste(c("Total lines processed:", LINE_PROCESSED), collapse = " "))
      corpus_batch <- VCorpus(VectorSource(batch))
      preprocessed <- preprocess(corpus_batch)
      for (j in 1:ngrams) {
        tokenizer = nGramTokenizer(j)
        dtm <- DocumentTermMatrix(preprocessed, list(tokenize = tokenizer, wordLengths=c(1, Inf)))
        keys <- colnames(dtm)
        require(slam)
        freq <- col_sums(dtm)
        updateCounts(h, keys, freq)
      }
      batch <- vector(mode = "character", length = batch_size)
      batch_i <- 1
    }
  }
  close(fcon)
  h
}

# process specified vector of training data, update n-gram count
freqCount2 <- function(h, lines, ngrams, batch_size) {
  TOTAL_LINES <- length(lines)
  batch_start <- 1
  end = FALSE
  while(TRUE) {
    batch_end <- batch_start + batch_size - 1
    if (batch_end >= TOTAL_LINES) {
      end = TRUE
      batch_end = TOTAL_LINES
    }
    #print(lines[batch_start:batch_end])
     corpus_batch <- VCorpus(VectorSource(lines[batch_start : batch_end]))
     preprocessed <- preprocess(corpus_batch)
      for (j in 1:ngrams) {
        tokenizer = nGramTokenizer(j)
        dtm <- DocumentTermMatrix(preprocessed, list(tokenize = tokenizer, wordLengths=c(1, Inf)))
        keys <- colnames(dtm)
        require(slam)
        freq <- col_sums(dtm)
        updateCounts(h[[j]], keys, freq)
      }
     if (end) {
       break
     } else {
       batch_start = batch_end + 1
     }
  }
  h
}

#freqCount2(h, 1:50, 4, 8)
# convert the hash into a trie to save space. One trie records word to probabilities, the other record word to word themselves
hash2trie <- function(h, n) {
  t <- trie(character(), numeric())
  t2 <- trie(character(), character())
  for (i in n:1) {
    keys <- keys(h)
    ngrams <- keys[sapply(str_split(keys, " "), length) == i]
    ngrams_freq <- valuesVector(ngrams, h)
    if (i == 1){
      TOP_UNIGRAM = ngrams[which.max(ngrams_freq)]
      TOP_UNIGRAM_FREQ = max(ngrams_freq)
      ngram_prop <- ngrams_freq / TOP_UNIGRAM_FREQ
    } else {
      parents <- trimEndWord(ngrams) 
      parents_freq <- valuesVector(parents, h)
      ngram_prop <- ngrams_freq / parents_freq
    }
    trie_add(t, ngrams, ngram_prop)
    trie_add(t2, ngrams, ngrams)
    del(ngrams, h)
  }
  list(trie.prob = t, trie.word = t2, ngram = n, top.unigram = TOP_UNIGRAM)
}

hash2trie2 <- function(h, n) {
  t <- trie(character(), numeric())
  t2 <- trie(character(), character())
  for (i in n:1) {
    ngrams <- keys(h[[i]])
    ngrams_freq <- valuesVector(ngrams, h[[i]])
    if (i == 1){
      TOP_UNIGRAM = ngrams[which.max(ngrams_freq)]
      TOP_UNIGRAM_FREQ = max(ngrams_freq)
      ngram_prop <- ngrams_freq / TOP_UNIGRAM_FREQ
    } else {
      parents <- trimEndWord(ngrams) 
      parents_freq <- valuesVector(parents, h[[i - 1]])
      ngram_prop <- ngrams_freq / parents_freq
    }
    trie_add(t, ngrams, ngram_prop)
    trie_add(t2, ngrams, ngrams)
    del(ngrams, h[[i]])
  }
  list(trie.prob = t, trie.word = t2, ngram = n, top.unigram = TOP_UNIGRAM)
}

train <- function(files, percent = 0.001, ngrams = 4, batch_size = 1000) {
  h <- hash()
  for (file in files) {
    fcon <- file(file, "r")
    subset <- vector(mode = "character")
    i = 0
    line = readLines(fcon, n = 1, skipNul = TRUE)
    while(length(line) != 0) {
      if (rbinom(1, 1, percent)) {
        subset[i] = line
        i = i + 1
      }
      line = readLines(fcon, n = 1, skipNul = TRUE)
    }
    close(fcon)
    write(subset, file = "capstone_temp.txt")
    
    h <- freqCount(h, "capstone_temp.txt" , ngrams, batch_size)
  }
  if (file.exists("capstone_temp.txt")){
    file.remove("capstone_temp.txt")
  }
  # h <- hash()
  # h <- freqCount(h, "final/en_subsets/twitter_subset.txt" , ngrams = 4, batch_size = 5000)
  # h <- freqCount(h, "final/en_subsets/news_subset.txt" , ngrams, batch_size)
  # h <- freqCount(h, "final/en_subsets/blogs_subset.txt" , ngrams, batch_size)
  model <- hash2trie(h, n = ngrams)
  rm(h)
  model
}

train2 <- function(files, percent = 0.001, ngrams = 4, batch_size = 1000) {
  h <- list()
  for (i in 1:ngrams) {
    h[[i]] = hash()
  }
  for (file in files) {
    fcon <- file(file, "r")
    alllines <- readLines(fcon, skipNul = TRUE)
    close(fcon)
    L = length(alllines)
    subset <- sample(alllines, floor(L * percent), replace = FALSE)
    rm(alllines)
    h <- freqCount2(h, subset , ngrams, batch_size)
    rm(subset)
  }
  model <- hash2trie2(h, n = ngrams)
  rm(h)
  model
}

Rprof()
model <- train(c(EN.TWITTER.PATH, EN.BLOGS.PATH, EN.NEWS.PATH), percent = 0.001, batch_size = 5000)

model <- train2(c(EN.TWITTER.PATH, EN.BLOGS.PATH, EN.NEWS.PATH), percent = 0.001, batch_size = 5000)


#h <- hash()
#h <- freqCount(h, EN.TWITTER.PATH, batch_size = 10000)
#h <- freqCount(h, "/Users/zhangx/git/datasciencecoursera/final/en_subsets/twitter_subset.txt",ngrams = 6)

# combined training method, compute all n-grams and their frequency in a trie structure
# training parameter is the percent, and max n of n-gram
# return a list containing the trie and n
# train <- function(percent = 0.01, n = 4) {
#     # subsetting data
#     corpus_sampling <- c(corpus_subset("twitter", percent), 
#                          corpus_subset("blogs", percent), 
#                          corpus_subset("news", percent))
#     # combining data
#     corpus_sampling <- str_c(corpus_sampling, collapse = " ")
#     corpus_sampling <- VCorpus(VectorSource(corpus_sampling))
#     # preprocess data
#     preprocessed <- preprocess(corpus_sampling)
#     # create unigram, bigram, ... n-gram
#     require(hash)
#     require(triebeard)
#     wordhash <- hash()
#     wordtrie <- trie(character(), numeric())
#     for (i in 1:n) {
#         tokenizer = nGramTokenizer(i)
#         dtm <- DocumentTermMatrix(preprocessed, list(tokenize = tokenizer, wordLengths=c(1, Inf)))
#         newWordList <- colnames(dtm)
#         newWordCount <- dtm$v
#         if (i == 1) {
#             total <- sum(newWordCount)
#             trie_add(wordtrie, keys = newWordList, values = newWordCount/total)
#             TOP_UNIGRAM = names(findMostFreqTerms(dtm)[[1]][1])
#         } else {
#             newWordParent <- trimEndWord(newWordList)
#             parentCount <- valuesVector(newWordParent, wordhash)
#             #print(newWordParent[! has.key(newWordParent, wordhash)])
#             if (length(newWordParent) != length(parentCount)) {
#               print(paste(c("newWordParent: ", length(newWordParent), " parentCount: ", length(parentCount)), collapse = " "))
#             }
#             trie_add(wordtrie, keys = newWordList, values = newWordCount/parentCount)
#         }
#         clear(wordhash) # just save keys that are required for next round
#         .set(wordhash, keys = newWordList, values = newWordCount)
#     }
#     rm(wordhash)
# 
#     list(trie = wordtrie, ngram = n, top.unigram = TOP_UNIGRAM)
# }


processSentence <- function(sentence) {
    sentence <- removePunctuation(sentence, preserve_intra_word_contractions = TRUE,
                  preserve_intra_word_dashes = TRUE,
                  ucp = TRUE)
    sentence <- removeNumbers(sentence)
    #sentence <- removeWords(sentence, stopwords("en"))
    sentence <- stripWhitespace(sentence)
    sentence <- tolower(sentence)
    sentence
}

trimLeadWord <- function(x) {
  tokens <- unlist(str_split(x, " "))
  LENGTH = length(tokens)
  if (LENGTH <= 1) {
    return ("")
  } else {
    return(str_c(tokens[2:LENGTH], collapse = " "))
  }
  
}

wordCount <- function(x){
  s <- str_split(x, " ")
  sapply(s, length)
}

predictScalar <- function(model, newData) {
  ## trim leading and trailing white spaces
  newData = trimws(newData)
  ## if this is an empty string, return the most frequent unigram
  if (str_count(newData) == 0) {
    return (model$top.unigram)
  } 
  ## else, use to frequency trie to make a prediction
  trie.prob <- model$trie.prob
  trie.word <- model$trie.word
  N <- model$ngram
  n <- length(unlist(str_split(newData, " ")))
  ## Being an N-gram model, the function will stop if the query word count is not between [1, N-1]
  if (n >= N) {
    stop("new data has more words than model can predict")
  } else {
    ## use regular expression to find candidate n+1 gram
    #candidatePosition <- str_detect(get_keys(wordtrie), paste(c("^", newData, " \\w+$"), collapse = ""))
    #candidatesKey <- get_keys(wordtrie)[candidatePosition]
    #candidatesValue <- get_values(wordtrie)[candidatePosition]
    matchedKeys <- unlist(prefix_match(trie.word, newData))
    matchedValues <- unlist(prefix_match(trie.prob, newData))
    ## arrange candidates by their frequency
    candidates <- data.frame(ngram = matchedKeys, prob = matchedValues) %>% filter(wordCount(ngram) == n + 1) %>%
      filter(str_detect(ngram, str_c(newData, " "))) %>% arrange(-prob)
    ## if we found at least one candidate, we return the most frequent one
    if (nrow(candidates) > 0) {
      #print(candidates)
      return (str_split(candidates[1,1], " ")[[1]][n + 1])
    } else { ## otherwise, we remove one word from the n-gram query, and make a recursive call
      return (predictScalar(model, trimLeadWord(newData)))
    }
  }
}

predictScalar(model, "at the")
predictScalar(model, "at")
predictScalar(model, "at the same")
predictScalar(model, "")
predictScalar(model, "jax has a")
# wordtrie <- trie(keys = c("A", "A a", "A b", "A c", "A a a", "B", "B a", "B b", "B c"), values <- c(5, 3, 1, 1, 1, 6, 2, 2, 2))
# trie_prob(wordtrie, 3)
predictNext <- function(model, sentences) {
  predictions <- vector(mode = "character", length = length(sentences))
  for (i in 1:length(sentences)) {
    processed <- processSentence(sentences[i])
    tokens = unlist(str_split(processed, " "))
    sentence_token_count = length(tokens)
    N = model$ngram
    if (sentence_token_count >= N) {
      tokens = tail(tokens, N - 1)
    }
    query = paste(tokens, collapse = " ")
    prediction = predictScalar(model, query)
    predictions[i] = prediction
  }
  predictions
}
predictNext(model, c("at the", "hello"))


#how to make it more efficient?
# 1. batch process files: read in 1000 lines, process and then move to the next 1000
# 2. count the N-gram frequencies, update the hash
# 3. move all hash into trie to save space
# 4. use trie for prediction
```

```{r}
quizes <- c("The guy in front of me just bought a pound of bacon, a bouquet, and a case of", 
               "You're the reason why I smile everyday. Can you follow me please? It would mean the", 
               "Hey sunshine, can you follow me and make me the",
               "Very early observations on the Bills game: Offense still struggling but the", 
               "Go on a romantic date at the", 
               "Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my", 
               "Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some", 
               "After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little", 
               "Be grateful for the good times and keep the faith during the", 
               "If this isn't the cutest thing you've ever seen, then you must be")
predictNext(model, quizes)
```
```{r}
quize2 <- c(
  "When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd", 
  "Guy at my table's wife got up to go to the bathroom and I asked about dessert and he started telling me about his",
  "I'd give anything to see arctic monkeys this",
  "Talking to your mom has the same effect as a hug and helps reduce your",
  "When you were in Holland you were like 1 inch away from me but you hadn't time to take a",
  "I'd just like all of these questions answered, a presentation of evidence, and a jury to settle the",
  "I can't deal with unsymetrical things. I can't even hold an uneven number of bags of groceries in each",
  "Every inch of you is perfect from the bottom to the",
  "Iâ€™m thankful my childhood was filled with imagination and bruises from playing",
  "I like how the same people are in almost all of Adam Sandler's")
predictNext(model, quize2)
```

